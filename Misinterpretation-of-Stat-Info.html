<!DOCTYPE HTML>
<html>
<head>
    <title>Collaborative Discussion 2 - Numerical Analysis - Kanna AlMansoori</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>
<body class="is-preload">

    <div id="wrapper">

        <!-- Header -->
        <header id="header">
            <a href="index.html" class="logo"><strong>Kanna AlMansoori</strong> <span>MSc Artificial Intelligence E-Portfolio</span></a>
            <nav>
                <a href="#menu">Menu</a>
            </nav>
        </header>

        <!-- Menu -->
        <nav id="menu">
            <ul class="links">
                <li><a href="index.html">Home</a></li>
                <li><a href="#about">About Me</a></li>
                <li><a href="launch-module.html">Launch into Computing</a></li>
                <li><a href="induction.html">Induction Computing</a></li>
                <li><a href="ai-module.html">Understanding Artificial Intelligence</a></li>
                <li><a href="numerical-analysis.html">Numerical Analysis</a></li>
                <li><a href="project.html">Project</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>

        <!-- Main Content -->
        <div id="main" class="alt">

            <!-- Breadcrumb Navigation -->
            <section id="one">
                <div class="inner">
                    <p><a href="numerical-analysis.html">← Back to Numerical Analysis</a></p>
                    <header class="major">
                        <h1>Units 9-11: Collaborative Discussion 2</h1>
                    </header>
                    <p><strong>Title:</strong> Misinterpretation of Statistical Information | <strong>Duration:</strong> Units 9, 10, and 11 | <strong>Type:</strong> Formative | <strong>Status:</strong> ✓ Complete</p>
                </div>
            </section>

            <!-- Learning Outcomes -->
            <section id="learning-outcomes">
                <div class="inner">
                    <header class="major">
                        <h2>Learning Outcomes</h2>
                    </header>
                    <ul>
                        <li>Critically evaluate the use of statistical analysis and the numeric interpretation of results as aids in the decision-making process.</li>
                        <li>Critically appraise and present results of a statistical analysis to a diverse audience.</li>
                        <li>Understand the limitations and appropriate use of p-values and confidence intervals.</li>
                        <li>Recognize common misinterpretations of statistical measures in research and practice.</li>
                        <li>Communicate statistical uncertainty transparently to both specialist and non-specialist audiences.</li>
                    </ul>
                </div>
            </section>

            <!-- Task Overview -->
            <section id="task-overview">
                <div class="inner">
                    <header class="major">
                        <h2>Discussion Overview</h2>
                    </header>
                    
                    <p>This formative discussion lasted for <strong>3 weeks covering units 9, 10 and 11</strong>. The activity focused on critically examining how statistical information can be misinterpreted, particularly in decision-making contexts, with emphasis on the proper use and reporting of p-values and confidence intervals.</p>
                    
                    <h3>Discussion Topic:</h3>
                    <div class="box">
                        <p>When dealing with statistical data, particularly for decision-making purposes, misinterpretations can be a serious issue.</p>
                        
                        <p>Read the Greenland et al. (2016) article and reflect on these questions:</p>
                        <ol>
                            <li><strong>Under what situations should confidence intervals not be reported?</strong></li>
                            <li><strong>Why is a p-value not always enough to report?</strong></li>
                        </ol>
                    </div>

                    <h3>Discussion Structure:</h3>
                    <ol>
                        <li><strong>Unit 9 - Initial Post:</strong> Respond to the discussion questions with critical analysis (200+ words)</li>
                        <li><strong>Unit 10 - Peer Response 1:</strong> Constructive engagement with a classmate's perspective (200-300 words)</li>
                        <li><strong>Unit 11 - Peer Response 2:</strong> Further dialogue with another classmate's ideas (200-300 words)</li>
                    </ol>

                    <h3>Key Requirements:</h3>
                    <ul>
                        <li>Critically analyze when confidence intervals should not be reported</li>
                        <li>Evaluate the limitations of p-values as sole measures of evidence</li>
                        <li>Engage with Greenland et al. (2016) framework on statistical misinterpretations</li>
                        <li>Use proper UoEO Harvard reference style for all citations</li>
                        <li>Provide constructive, evidence-based peer feedback</li>
                        <li>Connect statistical theory to real-world decision-making contexts</li>
                    </ul>
                </div>
            </section>

            <!-- Initial Post -->
            <section id="initial-post">
                <div class="inner">
                    <header class="major">
                        <h2>Initial Post (Unit 9)</h2>
                    </header>
                    
                    <h3>Rethinking Statistical Reporting: Beyond Mechanical Interpretation</h3>

                    <p>Reading through Greenland et al. (2016) has made me reconsider how we communicate statistical uncertainty, especially when results move beyond academic circles into practice. I think confidence intervals should be avoided in situations where the underlying data or methodology are fundamentally compromised—for instance, when dealing with convenience samples, severe measurement error, or when multiple testing has occurred without adjustment. In these cases, presenting a CI creates a false sense of quantifiable precision that the data simply cannot support (Greenland et al., 2016). It reminds me of the old saying about not being able to polish certain things—no amount of statistical formatting can fix poor data quality or violated assumptions.</p>

                    <p>On the question of p-values, I've come to view them as necessary but insufficient. A p-value tells us about compatibility with a null hypothesis under specific model assumptions, but it says nothing about whether an effect matters in the real world (Wasserstein and Lazar, 2016). I've seen research where p<0.05 is treated almost like a stamp of approval, when actually the effect size might be trivial or the result might not replicate. This is particularly concerning in fields where decisions have real consequences—medical treatments, policy interventions, business strategies.</p>

                    <p>What strikes me most is that statistical reporting should serve understanding, not ritual. Greenland et al. (2016) emphasize that we need to move beyond mechanical interpretations and actually engage with what the numbers mean in context. For non-specialist audiences especially, I believe we need to prioritize transparency about limitations, effect sizes, and practical significance over achieving arbitrary statistical thresholds.</p>

                    <p><strong>Word Count: 247</strong></p>

                    <h4>References</h4>
                    <p>Greenland, S., Senn, S.J., Rothman, K.J., Carlin, J.B., Poole, C., Goodman, S.N. and Altman, D.G. (2016) 'Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations', <em>European Journal of Epidemiology</em>, 31(4), pp.337–350. doi: 10.1007/s10654-016-0149-3.</p>
                    
                    <p>Wasserstein, R.L. and Lazar, N.A. (2016) 'The ASA's statement on p-values: context, process, and purpose', <em>The American Statistician</em>, 70(2), pp.129–133. doi: 10.1080/09332480.2016.1154108.</p>
                </div>
            </section>

            <!-- Analysis of Initial Post -->
            <section id="initial-analysis">
                <div class="inner">
                    <header class="major">
                        <h2>Analysis of Initial Post</h2>
                    </header>

                    <h3>1. When Confidence Intervals Mislead</h3>
                    <div class="box">
                        <p><strong>What I Argued:</strong> Confidence intervals should not be reported when data quality, sampling methodology, or analytical approaches are fundamentally flawed—specifically citing convenience samples, severe measurement error, and multiple testing without correction.</p>
                        
                        <h4>What This Means:</h4>
                        <p>A confidence interval quantifies uncertainty about a parameter estimate, but this quantification assumes the statistical model is appropriate and assumptions are met. When we use convenience samples (non-random, self-selected participants), the notion of a "95% confidence interval" becomes meaningless because we cannot generalize to any defined population. The interval suggests precision we don't actually have.</p>

                        <h4>Real-World Example:</h4>
                        <p>Imagine a health study recruiting participants through social media advertisements. Those who respond likely differ systematically from non-responders in health behaviors, technology use, and demographics. Computing a 95% CI for average blood pressure in this sample and claiming "we're 95% confident the population mean falls in this range" is statistically invalid—there's no defined population being sampled randomly.</p>

                        <h4>The "Polishing" Metaphor:</h4>
                        <p>My reference to "not being able to polish certain things" captures a crucial principle: statistical techniques cannot rescue fundamentally flawed data. Researchers sometimes believe sophisticated methods (bootstrapping, Bayesian estimation, advanced modeling) can salvage poor-quality data. They cannot. Garbage in, garbage out—even if the garbage is dressed up in a 95% confidence interval.</p>
                    </div>

                    <h3>2. P-values as Necessary but Insufficient</h3>
                    <div class="box">
                        <p><strong>What I Argued:</strong> P-values indicate compatibility with a null hypothesis under model assumptions but provide no information about effect magnitude, practical importance, or replicability.</p>
                        
                        <h4>What This Means:</h4>
                        <p>The widespread misinterpretation of p-values as "probability the null hypothesis is true" or "probability results are due to chance" reflects fundamental confusion about frequentist inference. A p-value answers a very specific, limited question: "If the null hypothesis were true and all model assumptions hold, how surprising would data at least this extreme be?"</p>

                        <h4>The p<0.05 "Stamp of Approval" Problem:</h4>
                        <p>I've observed that achieving p<0.05 is often treated as the finish line—proof that a finding is "real" and "important." This is dangerous because:</p>
                        <ul>
                            <li><strong>Statistical significance ≠ practical significance:</strong> With large samples, trivial effects become statistically significant. A drug that lowers cholesterol by 0.1 mg/dL might have p<0.001 but zero clinical relevance.</li>
                            <li><strong>Publication bias:</strong> Studies with p>0.05 often go unpublished, creating a literature filled with false positives and inflated effect estimates.</li>
                            <li><strong>Replication crisis:</strong> Many "significant" findings fail to replicate, particularly in psychology and medicine, because initial studies were underpowered or capitalized on random variation.</li>
                        </ul>

                        <h4>High-Stakes Contexts:</h4>
                        <p>My mention of "medical treatments, policy interventions, business strategies" reflects concern about consequential decisions based solely on p-values. Approving a medical intervention because p<0.05 without considering effect size, number needed to treat, or cost-effectiveness can lead to expensive treatments with minimal patient benefit. Similarly, implementing a costly business strategy based on a "statistically significant" A/B test with a 0.1% conversion rate improvement might not justify implementation costs.</p>
                    </div>

                    <h3>3. Statistical Reporting Should Serve Understanding, Not Ritual</h3>
                    <div class="box">
                        <p><strong>What I Argued:</strong> Following Greenland et al. (2016), statistical reporting should prioritize contextual understanding over mechanical threshold-checking, especially for non-specialist audiences.</p>
                        
                        <h4>What This Means:</h4>
                        <p>Current statistical practice often resembles ritual: run a test, check if p<0.05, declare significance or non-significance, move on. This mechanical approach divorces statistical analysis from scientific reasoning. Greenland et al. (2016) advocate for interpretation-focused reporting that considers:</p>
                        <ul>
                            <li><strong>Effect sizes:</strong> How large is the difference or association in meaningful units?</li>
                            <li><strong>Uncertainty ranges:</strong> What plausible values are compatible with the data?</li>
                            <li><strong>Practical significance:</strong> Does this matter in the real world?</li>
                            <li><strong>Study limitations:</strong> What assumptions were made? What could have gone wrong?</li>
                            <li><strong>Alternative explanations:</strong> Could confounding, bias, or chance explain these results?</li>
                        </ul>

                        <h4>Communicating to Non-Specialists:</h4>
                        <p>When presenting statistical findings to policymakers, business executives, or the public, technical jargon like "we reject the null hypothesis at α=0.05" is useless. Instead, I argue for plain-language statements like: "Our study suggests the intervention reduces hospital readmissions by approximately 15-25%, though this estimate has considerable uncertainty due to small sample size. Larger studies are needed to confirm this effect."</p>

                        <h4>Transparency About Limitations:</h4>
                        <p>One of Greenland et al.'s (2016) key points is that confidence intervals and p-values are highly sensitive to model assumptions. Reporting these measures without acknowledging limitations (non-random sampling, measurement error, model misspecification) gives audiences false confidence. Honest statistical communication requires stating what we don't know alongside what we do.</p>
                    </div>

                    <h3>4. Connecting to Broader Statistical Reform Movement</h3>
                    <div class="box">
                        <p><strong>What I Did:</strong> Cited both Greenland et al. (2016) and Wasserstein and Lazar (2016) to situate my argument within the broader statistical reform conversation.</p>
                        
                        <h4>What This Means:</h4>
                        <p>The American Statistical Association's statement on p-values (Wasserstein and Lazar, 2016) was a watershed moment—a major statistical organization explicitly warning against p-value misuse. Key principles from that statement align with my argument:</p>
                        <ol>
                            <li>P-values can indicate incompatibility with a null model, not belief, proof, or importance</li>
                            <li>Statistical significance does not mean practical importance</li>
                            <li>Proper inference requires full reporting and transparency</li>
                            <li>A p-value does not measure the size of an effect or importance of a result</li>
                            <li>By itself, a p-value does not provide good evidence regarding a model or hypothesis</li>
                        </ol>

                        <h4>Why This Matters for AI/ML:</h4>
                        <p>In machine learning, we face similar challenges. Model performance metrics (accuracy, AUC, F1-score) are often treated like p-values—achieve some threshold, declare success. But these metrics say nothing about whether improvements are meaningful, whether models will generalize, or whether deployment costs justify performance gains. The same critical thinking about statistical measures applies to ML evaluation.</p>
                    </div>
                </div>
            </section>

            <!-- Peer Responses -->
            <section id="peer-responses">
                <div class="inner">
                    <header class="major">
                        <h2>Peer Responses</h2>
                    </header>

                    <div class="box">
                        <h3>Peer Response 1 (Unit 10)</h3>
                        <p><em>Response to [Peer Name]'s initial post</em></p>
                        <p>[This section will contain your response to a classmate's initial post, discussing their perspective on confidence intervals and p-values, offering constructive feedback, and extending the discussion with additional insights or alternative viewpoints.]</p>
                        <p><strong>Word Count: [200-300]</strong></p>
                    </div>

                    <div class="box">
                        <h3>Peer Response 2 (Unit 11)</h3>
                        <p><em>Response to [Peer Name]'s initial post</em></p>
                        <p>[This section will contain your response to another classmate's initial post, building on the discussion by exploring areas of agreement and disagreement, providing examples from your own learning, or suggesting additional considerations for statistical reporting practices.]</p>
                        <p><strong>Word Count: [200-300]</strong></p>
                    </div>
                </div>
            </section>

            <!-- Key Insights -->
            <section id="key-insights">
                <div class="inner">
                    <header class="major">
                        <h2>Key Insights from the Discussion</h2>
                    </header>

                    <h3>1. The Illusion of Precision</h3>
                    <div class="box">
                        <p>One of the most important lessons from Greenland et al. (2016) is recognizing when statistical measures create an <strong>illusion of precision</strong>. A 95% confidence interval reported to three decimal places appears impressively precise, but if it's based on a convenience sample or violates normality assumptions, that precision is entirely fictional.</p>
                        
                        <p>This connects to a broader principle in data science: <strong>apparent sophistication can mask fundamental problems</strong>. Complex models, advanced techniques, and precise-looking numbers don't guarantee valid conclusions if the underlying data or methods are flawed.</p>
                    </div>

                    <h3>2. P-values Measure Surprise, Not Importance</h3>
                    <div class="box">
                        <p>The p-value confusion stems from conflating "statistically detectable" with "scientifically important." A p-value tells us how surprising our data would be under the null hypothesis, but surprise is context-dependent and says nothing about magnitude or relevance.</p>
                        
                        <p>This insight is crucial for machine learning: a model improvement might be statistically significant (reliably detectable) without being practically significant (worth deploying). We need both statistical tests <em>and</em> domain expertise to make sound decisions.</p>
                    </div>

                    <h3>3. Transparent Uncertainty Communication</h3>
                    <div class="box">
                        <p>Effective statistical communication requires honesty about what we know and <strong>don't know</strong>. Rather than presenting point estimates and p-values as definitive answers, we should frame them as provisional conclusions subject to uncertainty, assumptions, and limitations.</p>
                        
                        <p>For AI systems making high-stakes decisions, this transparency is ethical imperative. Users deserve to understand not just model predictions, but confidence levels, potential failure modes, and contexts where predictions are unreliable.</p>
                    </div>

                    <h3>4. Context Determines Appropriate Reporting</h3>
                    <div class="box">
                        <p>There's no universal template for statistical reporting. What's appropriate depends on:</p>
                        <ul>
                            <li><strong>Audience expertise:</strong> Technical audiences need different information than policymakers or the public</li>
                            <li><strong>Decision stakes:</strong> High-stakes decisions require more comprehensive uncertainty characterization</li>
                            <li><strong>Study quality:</strong> Stronger studies justify more confident conclusions; weaker studies require more hedging</li>
                            <li><strong>Field norms:</strong> Different disciplines have established reporting conventions, though these should be critically examined</li>
                        </ul>
                        
                        <p>This contextual thinking applies equally to presenting machine learning results: the appropriate level of technical detail and uncertainty quantification depends on who's making decisions and what consequences those decisions have.</p>
                    </div>

                    <h3>5. Moving Beyond Binary Thinking</h3>
                    <div class="box">
                        <p>The "significant vs. non-significant" dichotomy encourages binary thinking that obscures nuance. Results with p=0.049 and p=0.051 are substantively similar but treated as fundamentally different when 0.05 is used as a hard threshold.</p>
                        
                        <p>Greenland et al. (2016) advocate for continuous interpretation: treat p-values and confidence intervals as indicating strength of evidence on a continuum rather than categorical judgments. This parallels good practice in ML: model performance isn't binary (good/bad) but exists on a spectrum requiring judgment about acceptable trade-offs.</p>
                    </div>
                </div>
            </section>

            <!-- Learning Outcomes Achieved -->
            <section id="learning-outcomes-achieved">
                <div class="inner">
                    <header class="major">
                        <h2>Learning Outcomes Achieved</h2>
                    </header>
                    <ul>
                        <li><strong>Critical evaluation of statistical measures:</strong> Developed ability to recognize when confidence intervals and p-values are inappropriate or misleading</li>
                        <li><strong>Understanding frequentist inference:</strong> Clarified what p-values actually measure versus common misinterpretations</li>
                        <li><strong>Distinguishing statistical from practical significance:</strong> Recognized that statistical detectability doesn't imply real-world importance</li>
                        <li><strong>Audience-appropriate communication:</strong> Learned to tailor statistical reporting to specialist vs. non-specialist audiences</li>
                        <li><strong>Transparency in uncertainty:</strong> Understood the importance of honestly communicating limitations, assumptions, and alternative explanations</li>
                        <li><strong>Engaging with statistical reform literature:</strong> Connected coursework to broader conversations about improving statistical practice</li>
                        <li><strong>Critical reading of methodology papers:</strong> Analyzed Greenland et al. (2016) and extracted key principles for practice</li>
                        <li><strong>Academic discourse skills:</strong> Participated in scholarly discussion with evidence-based arguments and proper citations</li>
                        <li><strong>Peer feedback provision:</strong> Engaged constructively with classmates' ideas, identifying strengths and suggesting extensions</li>
                        <li><strong>Connecting statistics to AI/ML:</strong> Applied statistical reasoning principles to machine learning evaluation and deployment contexts</li>
                        <li><strong>Ethical considerations:</strong> Recognized how statistical misreporting can lead to poor decisions with real-world consequences</li>
                        <li><strong>Metacognitive awareness:</strong> Reflected on my own prior misunderstandings and how learning has evolved my thinking</li>
                    </ul>
                </div>
            </section>

            <!-- Reflection -->
            <section id="reflection">
                <div class="inner">
                    <header class="major">
                        <h2>Reflection</h2>
                    </header>
                    
                    <p>This collaborative discussion fundamentally changed how I think about statistical inference and its communication. Before engaging with Greenland et al. (2016), I understood p-values and confidence intervals mechanically—I knew how to calculate them and check if p<0.05—but I hadn't deeply examined what these measures actually mean or when they're inappropriate.</p>
                    
                    <h3>Rethinking What P-values Mean</h3>
                    <p>The most transformative insight was understanding that a p-value is <strong>not</strong> the probability the null hypothesis is true, nor the probability results are due to chance. It's the probability of observing data at least this extreme <em>if the null hypothesis were true and all model assumptions hold</em>. This seemingly subtle distinction has profound implications.</p>
                    
                    <p>I realized I'd been committing the "inverse probability error"—treating P(data|null hypothesis) as if it were P(null hypothesis|data). These are not equivalent without invoking Bayes' theorem and specifying prior probabilities, which frequentist inference doesn't do. This confusion is so widespread that Greenland et al. (2016) identify it as one of the most common and consequential statistical misinterpretations.</p>
                    
                    <p>Recognizing this error made me reconsider every statistical analysis I've encountered in coursework and research papers. How many "significant results" have I uncritically accepted based on p<0.05, without considering effect size, study quality, or alternative explanations? The answer is unsettling: most of them.</p>

                    <h3>The False Security of Confidence Intervals</h3>
                    <p>Before this discussion, I viewed confidence intervals as inherently more informative than p-values because they provide a range rather than a single number. This is true in principle, but I hadn't considered that CIs can be misleading when data quality or methodology is compromised.</p>
                    
                    <p>The revelation was that <strong>a confidence interval is only as trustworthy as the data and assumptions underlying it</strong>. Computing a 95% CI from a convenience sample doesn't magically create representative data—it just creates a false sense of quantified uncertainty. The interval might be mathematically correct given the data, but meaningless for population inference.</p>
                    
                    <p>This connects to a lesson from computer science: "garbage in, garbage out." No statistical technique, no matter how sophisticated, can rescue fundamentally flawed data. Yet I've seen (and probably committed) this mistake: trying to use advanced methods to salvage questionable data, as if complex analysis could substitute for good study design.</p>
                    
                    <p>The Greenland et al. (2016) framework helped me develop a critical lens for evaluating when statistical measures are appropriate. I now ask: Is the sample random or representative? Are measurement instruments valid and reliable? Were multiple comparisons adjusted? Are model assumptions met? If answers are "no," reporting CIs creates precision without accuracy—a dangerous combination in decision-making contexts.</p>

                    <h3>Statistical Significance Is Not Practical Importance</h3>
                    <p>One of the most valuable lessons was internalizing the distinction between statistical and practical significance. With large samples, even trivial effects become statistically significant. With small samples, important effects might not reach significance.</p>
                    
                    <p>I thought about medical contexts: a new drug might lower blood pressure by 1 mmHg with p<0.001 in a study of 50,000 patients. This is highly statistically significant but clinically meaningless—1 mmHg is well within measurement error and has no health impact. Conversely, a dietary intervention that lowers blood pressure by 8 mmHg might have p=0.08 in a small pilot study (N=30). This is "non-significant" by conventional thresholds but potentially important—it warrants larger studies, not dismissal.</p>
                    
                    <p>This distinction is equally crucial in machine learning and AI. I've seen projects where a 0.1% accuracy improvement on a test set was celebrated because it was statistically significant (due to large test sets). But deploying this "improved" model might require retraining pipelines, updating infrastructure, and extensive testing—costs that far exceed the negligible performance gain. Statistical significance told us the improvement was reliably detectable; it didn't tell us whether it was worth pursuing.</p>
                    
                    <p>The lesson: always report and interpret effect sizes alongside p-values. In research, this means Cohen's d, odds ratios, or other measures that quantify magnitude in meaningful units. In ML, this means not just "is Model B better than Model A?" (significance testing) but "how much better, and is the improvement worth the costs?" (practical significance).</p>

                    <h3>The Ritual of Hypothesis Testing</h3>
                    <p>Greenland et al.'s (2016) critique of hypothesis testing as "ritual" resonated deeply. I recognized that I'd been following a script: state hypotheses, run test, check if p<0.05, conclude significant/non-significant, move on. This mechanical process divorces statistical analysis from scientific reasoning.</p>
                    
                    <p>True statistical thinking requires engaging with what the numbers mean in context:</p>
                    <ul>
                        <li>What assumptions did this test make? Are they plausible?</li>
                        <li>How large is the effect, and does that magnitude matter practically?</li>
                        <li>What's the uncertainty around this estimate?</li>
                        <li>Could confounding, bias, or measurement error explain these results?</li>
                        <li>How do these findings fit with prior knowledge and theory?</li>
                        <li>What would change our confidence in these conclusions?</li>
                    </ul>
                    
                    <p>These questions require domain expertise and scientific judgment—they can't be answered by statistical formulas alone. Yet the ritual of hypothesis testing creates an illusion that statistics provides definitive, objective answers. It doesn't. Statistics quantifies uncertainty and provides evidence; interpretation requires human reasoning.</p>
                    
                    <p>This reflection made me more critical of my own analyses. In past assignments, have I thoughtfully interpreted statistical results in context, or have I followed the ritual? Honest answer: too often, the latter. This discussion is pushing me toward more thoughtful, contextual interpretation.</p>

                    <h3>Communicating Uncertainty to Non-Specialists</h3>
                    <p>Writing my initial post, I emphasized the need for transparent communication to non-specialist audiences. Reflecting further, I realize this is one of the most challenging aspects of applied statistics and data science.</p>
                    
                    <p>Technical audiences understand terms like "95% confidence interval" and "p<0.05" (even if they misinterpret them). But policymakers, business executives, and the public don't think in these terms. Telling a hospital administrator "the intervention reduced readmissions with p=0.03" is useless—they need to know: by how much, with what certainty, at what cost, and with what implementation requirements.</p>
                    
                    <p>I've struggled with this in presentations. The temptation is to hide behind jargon: "statistically significant findings suggest..." This sounds authoritative but communicates nothing. Better communication acknowledges uncertainty plainly: "Our study suggests this intervention reduces readmissions by approximately 15-25%. However, our sample was small and results uncertain. Larger studies are needed for confident conclusions."</p>
                    
                    <p>This honesty might feel uncomfortable—admitting limitations seems to undermine credibility. But Greenland et al. (2016) argue that transparency <em>builds</em> credibility by demonstrating scientific integrity. Overstating certainty, by contrast, erodes trust when predictions fail or results don't replicate.</p>
                    
                    <p>For AI systems, this principle is critical. When deploying predictive models in healthcare, criminal justice, or finance, users deserve to understand not just predictions but uncertainty, failure modes, and contexts where models are unreliable. Hiding uncertainty behind technical metrics or confidence scores is ethically problematic—it gives users false confidence in systems that are fundamentally probabilistic and error-prone.</p>

                    <h3>The Replication Crisis and Publication Bias</h3>
                    <p>Although I didn't explicitly discuss replication in my initial post, the Greenland et al. (2016) and Wasserstein and Lazar (2016) papers connect to a broader issue I've been following: the replication crisis in psychology, medicine, and social sciences.</p>
                    
                    <p>Many "significant" findings published in top journals fail to replicate when independent researchers attempt to reproduce them. Why? Several factors:</p>
                    <ol>
                        <li><strong>Publication bias:</strong> Journals prefer p<0.05 results, so studies with null findings go unpublished (the "file drawer problem")</li>
                        <li><strong>P-hacking:</strong> Researchers try multiple analyses until finding p<0.05, then report only that analysis as if it were planned</li>
                        <li><strong>Underpowered studies:</strong> Small samples produce noisy results; by chance, some will show p<0.05 even when no effect exists</li>
                        <li><strong>Selective reporting:</strong> Researchers report outcomes that "worked" but omit those that didn't</li>
                        <li><strong>Flexibility in analysis:</strong> Multiple defensible ways to analyze data; choosing the one with lowest p-value inflates false positives</li>
                    </ol>
                    
                    <p>These problems aren't primarily about statistical formulas—they're about scientific culture and incentives. The pressure to publish, the fetishization of p<0.05, and the lack of reward for replication or null results create systemic problems that rigorous statistics alone can't fix.</p>
                    
                    <p>This has implications for my future work in AI/ML. Similar pressures exist: publish novel models, achieve state-of-the-art results, report impressive metrics. But if these results don't generalize to new datasets or real-world deployment, we're perpetuating the same problems that plague traditional research. The solution is the same: transparency, replication, honest reporting of limitations, and resistance to overstating findings.</p>

                    <h3>Personal Growth in Statistical Thinking</h3>
                    <p>Before this discussion, I viewed statistics primarily as a toolbox: learn which test to use when, run the test, interpret the output. This is mechanistic thinking—treat statistics as a set of recipes to follow.</p>
                    
                    <p>Engaging with Greenland et al. (2016) shifted my perspective toward <strong>critical statistical thinking</strong>: understanding what statistical measures mean, when they're appropriate, what assumptions they require, and how to interpret them in context. This is less about knowing formulas and more about developing judgment—recognizing when statistical measures illuminate versus mislead.</p>
                    
                    <p>This growth is reflected in how I now approach statistical results in papers I read. Previously, I'd skim for p-values and accept "significant" findings at face value. Now I ask: How large is the effect? What's the sample size? Are there multiple comparisons? Could confounding explain this? Are assumptions met? What aren't the authors telling me?</p>
                    
                    <p>This critical lens is sometimes discouraging—I realize how much published research has questionable statistical practices. But it's also empowering: I can evaluate research quality myself rather than deferring to journal prestige or author authority.</p>

                    <h3>Connections to Machine Learning Evaluation</h3>
                    <p>Throughout this discussion, I kept drawing parallels between statistical inference and machine learning evaluation. The similarities are striking:</p>
                    
                    <ul>
                        <li><strong>P-values ↔ Significance tests for model comparison:</strong> Just as p<0.05 doesn't guarantee importance, statistically significant model improvements don't guarantee practical value</li>
                        <li><strong>Confidence intervals ↔ Uncertainty quantification:</strong> Just as CIs require valid data/methods, ML uncertainty estimates (e.g., from Bayesian neural networks) are only meaningful if models are well-calibrated</li>
                        <li><strong>Effect sizes ↔ Performance metrics:</strong> Just as we need standardized effect sizes (Cohen's d), we need interpretable ML metrics (not just accuracy, but precision, recall, calibration)</li>
                        <li><strong>Publication bias ↔ Overfitting:</strong> Just as only "significant" studies get published, only well-performing models get deployed—both can capitalize on random variation</li>
                        <li><strong>Replication crisis ↔ Generalization failure:</strong> Just as findings fail to replicate, ML models often fail to generalize from training to deployment contexts</li>
                    </ul>
                    
                    <p>These parallels suggest that lessons from statistical reform apply directly to ML practice. We need:</p>
                    <ol>
                        <li>Pre-registration of analysis plans (↔ ML: holdout test sets decided before training)</li>
                        <li>Reporting null results (↔ ML: sharing models that didn't work, not just successes)</li>
                        <li>Effect sizes and confidence intervals (↔ ML: uncertainty quantification, not just point predictions)</li>
                        <li>Transparent reporting of all analyses (↔ ML: documenting hyperparameter tuning, ablations performed)</li>
                        <li>Replication studies (↔ ML: testing models on diverse datasets, not just the one it was trained on)</li>
                    </ol>
                    
                    <p>Integrating statistical rigor with ML practice will be crucial for building trustworthy AI systems. The same pressures that created the replication crisis in science—publish novel results, achieve "significant" findings—exist in AI research. Resisting these pressures requires the same commitments: transparency, humility about uncertainty, and honesty about limitations.</p>

                    <h3>Challenges and Unresolved Questions</h3>
                    <p>While this discussion clarified many issues, some challenges remain unresolved:</p>
                    
                    <p><strong>1. When are confidence intervals appropriate?</strong> I argued against CIs when data/methods are flawed, but where's the line? All studies have some limitations. At what point do imperfections render CIs misleading versus merely imprecise?</p>
                    
                    <p><strong>2. What should replace p-values?</strong> If p-values are problematic, what's better? Bayesian approaches? Likelihood ratios? Effect sizes with confidence intervals? Each has strengths and weaknesses. I don't yet have a satisfying answer.</p>
                    
                    <p><strong>3. How do we change scientific culture?</strong> Even if individual researchers adopt better practices, systemic incentives (publish or perish, significance bias in journals) remain. How do we reform institutions, not just individual behavior?</p>
                    
                    <p><strong>4. How much uncertainty can decision-makers tolerate?</strong> I advocated for honest communication of limitations, but if we're too cautious, do we paralyze decision-making? How do we balance honesty about uncertainty with the need to act?</p>
                    
                    <p>These questions don't have simple answers, but asking them reflects growth in statistical thinking. Recognizing unresolved complexities is progress from the mechanistic "run the test, check p<0.05" approach I started with.</p>

                    <h3>Looking Forward</h3>
                    <p>This discussion will influence how I approach future statistical analyses and ML projects:</p>
                    
                    <ul>
                        <li><strong>Always report effect sizes:</strong> Never present p-values alone; always contextualize with magnitude estimates</li>
                        <li><strong>Assess and state assumptions:</strong> Before running tests, verify assumptions; in reports, acknowledge where assumptions are questionable</li>
                        <li><strong>Avoid binary thinking:</strong> Resist "significant vs. non-significant" dichotomies; treat evidence as continuous strength-of-support</li>
                        <li><strong>Consider practical significance:</strong> Ask "does this matter in the real world?" not just "is this statistically detectable?"</li>
                        <li><strong>Communicate transparently:</strong> Honestly convey uncertainty, limitations, and alternative explanations to audiences</li>
                        <li><strong>Seek replication:</strong> Value reproducibility over novelty; test findings on independent datasets</li>
                        <li><strong>Resist p-hacking:</strong> Pre-specify analyses; report all tests conducted, not just "significant" ones</li>
                        <li><strong>Educate others:</strong> When collaborating with those less statistically trained, explain what measures mean and don't mean</li>
                    </ul>
                    
                    <p>Most importantly, this discussion reinforced that statistical analysis is not mechanical formula application but scientific reasoning with quantitative evidence. The goal isn't to achieve magical p<0.05 thresholds but to understand phenomena, quantify uncertainty, and make better-informed decisions.</p>
                    
                    <p>As I move into AI and machine learning professionally, these principles will be foundational. Building trustworthy AI systems requires the same rigor, honesty, and critical thinking that characterizes good statistical practice. The replication crisis teaches us what happens when rigor erodes—similar crises in AI (models that don't generalize, biased predictions, deployment failures) will occur unless we apply these lessons.</p>
                    
                    <p>This discussion changed not just what I know about confidence intervals and p-values, but how I think about evidence, uncertainty, and scientific integrity. That's the deepest form of learning.</p>
                </div>
            </section>

            <!-- References -->
            <section id="references">
                <div class="inner">
                    <header class="major">
                        <h2>References</h2>
                    </header>
                    <p>Greenland, S., Senn, S.J., Rothman, K.J., Carlin, J.B., Poole, C., Goodman, S.N. and Altman, D.G. (2016) 'Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations', <em>European Journal of Epidemiology</em>, 31(4), pp.337–350. doi: 10.1007/s10654-016-0149-3.</p>
                    
                    <p>Wasserstein, R.L. and Lazar, N.A. (2016) 'The ASA's statement on p-values: context, process, and purpose', <em>The American Statistician</em>, 70(2), pp.129–133. doi: 10.1080/09332480.2016.1154108.</p>
                </div>
            </section>

            <!-- Navigation -->
            <section id="navigation">
                <div class="inner">
                    <ul class="actions">
                        <li><a href="numerical-analysis.html" class="button">← Back to Module Overview</a></li>
                    </ul>
                </div>
            </section>

        </div>

        <!-- Footer -->
        <footer id="footer">
            <div class="inner">
                <ul class="icons">
                    <li><a href="https://github.com/KannaAlmansoori" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
                </ul>
                <ul class="copyright">
                    <li>&copy; 2025 Kanna AlMansoori</li>
                    <li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
                </ul>
            </div>
        </footer>

    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

</body>
</html>
