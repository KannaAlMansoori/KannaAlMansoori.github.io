html_content = """<!DOCTYPE HTML>
<html>
<head>
    <title>Unit 4 Probability Activities - Numerical Analysis - Kanna AlMansoori</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>
<body class="is-preload">

    <div id="wrapper">

        <!-- Header -->
        <header id="header">
            <a href="index.html" class="logo"><strong>Kanna AlMansoori</strong> <span>MSc Artificial Intelligence E-Portfolio</span></a>
            <nav>
                <a href="#menu">Menu</a>
            </nav>
        </header>

        <!-- Menu -->
        <nav id="menu">
            <ul class="links">
                <li><a href="index.html">Home</a></li>
                <li><a href="#about">About Me</a></li>
                <li><a href="launch-module.html">Launch into Computing</a></li>
                <li><a href="induction.html">Induction Computing</a></li>
                <li><a href="ai-module.html">Understanding Artificial Intelligence</a></li>
                <li><a href="numerical-analysis.html">Numerical Analysis</a></li>
                <li><a href="project.html">Project</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>

        <!-- Main Content -->
        <div id="main" class="alt">

            <!-- Breadcrumb Navigation -->
            <section id="one">
                <div class="inner">
                    <p><a href="numerical-analysis.html">← Back to Numerical Analysis</a></p>
                    <header class="major">
                        <h1>Unit 4: Probability Activities</h1>
                    </header>
                    <p><strong>Title:</strong> Exploring Fundamental Probability Concepts and Distributions | <strong>Deadline:</strong> End of Unit 4 | <strong>Type:</strong> Formative | <strong>Status:</strong> ✓ Complete</p>
                </div>
            </section>

            <!-- Learning Outcomes -->
            <section id="learning-outcomes">
                <div class="inner">
                    <header class="major">
                        <h2>Learning Outcomes from This Activity</h2>
                    </header>
                    <ul>
                        <li>Systematic understanding of the key mathematical and statistical concepts and techniques which underpin mechanisms in Data Science and AI.</li>
                        <li>Apply mathematical and statistical methods in these fields to help in the decision-making process.</li>
                        <li>Critically evaluate the use of statistical analysis and the numeric interpretation of results as aids in the decision-making process.</li>
                    </ul>
                </div>
            </section>

            <!-- Task Overview -->
            <section id="task-overview">
                <div class="inner">
                    <header class="major">
                        <h2>Task Overview</h2>
                    </header>
                    
                    <p>This unit introduced foundational probability concepts through three hands-on activities that build understanding from basic calculations to complex distributions.</p>
                    
                    <div class="box">
                        <h3>Three Activities Completed:</h3>
                        <ol>
                            <li><strong>Activity 1: Marble Probability</strong> - Calculate probabilities for selecting colored marbles and verify they sum to 1</li>
                            <li><strong>Activity 2: Real-World Applications</strong> - Watch videos on crime prediction and sampling methods</li>
                            <li><strong>Activity 3: Probability Distributions</strong> - Explore discrete and continuous distributions interactively</li>
                        </ol>
                    </div>
                </div>
            </section>

            <!-- ============================================ -->
            <!-- ACTIVITY 1: MARBLE PROBABILITY -->
            <!-- ============================================ -->
            <section id="activity1">
                <div class="inner">
                    <header class="major">
                        <h2>Activity 1: Marble Probability</h2>
                    </header>

                    <h3>Methodology</h3>
                    <p>Using the Wolfram demonstration website, I worked with an interactive marble probability simulator. The scenario had:</p>
                    <ul>
                        <li>1 red marble</li>
                        <li>1 blue marble</li>
                        <li>1 yellow marble</li>
                        <li><strong>Total: 3 marbles</strong></li>
                    </ul>
                    <p><strong>Probability Formula:</strong> P(event) = Number of favorable outcomes / Total number of possible outcomes</p>
                    <p>I calculated the probability of drawing each color, then verified that all probabilities sum to 1.</p>

                    <h3>Results</h3>
                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Question</th>
                                    <th>Calculation</th>
                                    <th>Answer</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Probability of getting a red marble</td>
                                    <td>P(red) = 1/3</td>
                                    <td>0.333 or 33.33%</td>
                                </tr>
                                <tr>
                                    <td>Probability of getting a blue marble</td>
                                    <td>P(blue) = 1/3</td>
                                    <td>0.333 or 33.33%</td>
                                </tr>
                                <tr>
                                    <td>Probability of getting a yellow marble</td>
                                    <td>P(yellow) = 1/3</td>
                                    <td>0.333 or 33.33%</td>
                                </tr>
                                <tr style="background-color: #f5f5f5;">
                                    <td><strong>Verification (sum of all probabilities)</strong></td>
                                    <td>1/3 + 1/3 + 1/3</td>
                                    <td><strong>1.0 or 100% ✓</strong></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h3>Analysis and Key Findings</h3>
                    <div class="box">
                        <p>This simple exercise demonstrates fundamental probability axioms:</p>
                        <ul>
                            <li><strong>Non-negativity:</strong> All probabilities are between 0 and 1</li>
                            <li><strong>Normalization:</strong> Sum of all probabilities equals 1</li>
                            <li><strong>Uniformity:</strong> With equal chances, each outcome has probability 1/n</li>
                        </ul>
                        <p><strong>Why this matters for AI:</strong> These simple rules constrain all probability models. When machine learning models output probabilities that don't sum to 1, that's a red flag indicating something is wrong.</p>
                    </div>
                </div>
            </section>

            <!-- ============================================ -->
            <!-- ACTIVITY 2: REAL-WORLD APPLICATIONS -->
            <!-- ============================================ -->
            <section id="activity2">
                <div class="inner">
                    <header class="major">
                        <h2>Activity 2: Real-World Applications</h2>
                    </header>

                    <h3>Methodology</h3>
                    <p>Watched two educational videos to understand how probability is applied in real-world contexts:</p>
                    <ul>
                        <li><strong>Video 1: "Crime spotting: Joy of Stats"</strong> - Demonstrated everyday applications of probability in society, including predictive policing and risk assessment</li>
                        <li><strong>Video 2: "Types of Sampling Methods"</strong> - Explained different statistical sampling techniques</li>
                    </ul>
                    <p>I took notes on key concepts and reflected on how these applications connect to data science and AI.</p>

                    <h3>Results</h3>
                    <div class="box">
                        <h4>Crime Prediction - Key Insights:</h4>
                        <ul>
                            <li><strong>Predictive Policing:</strong> Using probability to forecast where crimes are most likely to occur</li>
                            <li><strong>Resource Allocation:</strong> Deploying police resources based on probability distributions of crime locations</li>
                            <li><strong>Risk Assessment:</strong> Calculating probabilities of different crime types in various areas</li>
                            <li><strong>Pattern Recognition:</strong> Using historical data to identify probability patterns</li>
                        </ul>
                    </div>

                    <div class="box">
                        <h4>Sampling Methods Understood:</h4>
                        <ul>
                            <li><strong>Simple Random Sampling:</strong> Every member has equal probability of selection</li>
                            <li><strong>Stratified Random Sampling:</strong> Dividing population into groups (strata) and randomly sampling from each</li>
                            <li><strong>Systematic Sampling:</strong> Selecting every nth item from a list</li>
                            <li><strong>Cluster Sampling:</strong> Dividing population into clusters and randomly selecting entire clusters</li>
                        </ul>
                    </div>

                    <h3>Analysis and Key Findings</h3>
                    <div class="box">
                        <p><strong>Crime Prediction Insight:</strong> This video showed probability's real impact. Using historical data to predict crime locations is the same approach used in customer churn prediction, equipment failure forecasting, and medical diagnosis—all examples of predictive analytics.</p>
                        
                        <p><strong>Ethical Considerations:</strong> However, the video also raised important questions. Using biased historical data can create self-fulfilling prophecies and reinforce existing biases. This taught me that data scientists can't just optimize metrics—we must think critically about societal impacts.</p>
                        
                        <p><strong>Sampling Is Critical:</strong> The sampling video made clear that <strong>bad sampling = biased model</strong>. If training data isn't representative due to poor sampling, the model will fail in production. Stratified sampling ensures all groups are represented—crucial for building fair AI systems.</p>
                    </div>
                </div>
            </section>

            <!-- ============================================ -->
            <!-- ACTIVITY 3: PROBABILITY DISTRIBUTIONS -->
            <!-- ============================================ -->
            <section id="activity3">
                <div class="inner">
                    <header class="major">
                        <h2>Activity 3: Probability Distributions</h2>
                    </header>

                    <h3>Methodology</h3>
                    <p>Used interactive Jupyter notebook manipulatives to explore both discrete and continuous probability distributions. For each distribution, I adjusted parameters and observed how the distribution shape and properties changed.</p>

                    <h4>Discrete Distributions Explored:</h4>
                    <div class="box">
                        <h5>1. Binomial Distribution</h5>
                        <p>Models the number of successes in a fixed number of independent trials.</p>
                        <ul>
                            <li><strong>Parameters:</strong> n (number of trials), p (probability of success)</li>
                            <li><strong>Setup:</strong> Coin toss simulation with n=10 tosses, p=0.5</li>
                        </ul>
                    </div>

                    <div class="box">
                        <h5>2. Poisson Distribution</h5>
                        <p>Models the number of events occurring in a fixed interval when events occur independently at a constant rate.</p>
                        <ul>
                            <li><strong>Parameter:</strong> λ (lambda) = mean number of events per unit time</li>
                            <li><strong>Setup:</strong> Event counting simulation with confidence intervals</li>
                        </ul>
                    </div>

                    <div class="box">
                        <h5>3. Hypergeometric Distribution</h5>
                        <p>Models sampling without replacement from a finite population.</p>
                        <ul>
                            <li><strong>Parameters:</strong> N=100 (total balls), k=25 (marked balls), n=20 (sample size)</li>
                            <li><strong>Formula:</strong> Mean = kn/N, Variance = kn(1-k/N)(-n+N)/[(-1+N)N]</li>
                        </ul>
                    </div>

                    <h4>Continuous Distributions Explored:</h4>
                    <div class="box">
                        <h5>1. Normal (Gaussian) Distribution</h5>
                        <p>The most important continuous distribution, characterized by its bell-shaped curve.</p>
                        <ul>
                            <li><strong>Parameters:</strong> μ (mean - center), σ (standard deviation - spread)</li>
                            <li><strong>Formula:</strong> f(y) = (1/√(2πσ²)) × e^(-(y-μ)²/(2σ²))</li>
                        </ul>
                    </div>

                    <div class="box">
                        <h5>2. Exponential Distribution</h5>
                        <p>Models waiting times between events in a Poisson process.</p>
                        <ul>
                            <li><strong>Parameter:</strong> λ (rate parameter, must be positive)</li>
                            <li><strong>Properties:</strong> Mean = 1/λ, Variance = 1/λ²</li>
                            <li><strong>Key Feature:</strong> Memoryless property</li>
                        </ul>
                    </div>

                    <h3>Results</h3>
                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Distribution</th>
                                    <th>Type</th>
                                    <th>Key Observation</th>
                                    <th>Common Use in AI/ML</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Binomial</td>
                                    <td>Discrete</td>
                                    <td>Symmetric when p=0.5</td>
                                    <td>A/B testing, classification metrics</td>
                                </tr>
                                <tr>
                                    <td>Poisson</td>
                                    <td>Discrete</td>
                                    <td>Confidence intervals widen over time</td>
                                    <td>Anomaly detection, event counting</td>
                                </tr>
                                <tr>
                                    <td>Hypergeometric</td>
                                    <td>Discrete</td>
                                    <td>Mean = 5.00, Variance = 3.03</td>
                                    <td>Sampling without replacement</td>
                                </tr>
                                <tr>
                                    <td>Normal</td>
                                    <td>Continuous</td>
                                    <td>Adjusting σ changes spread</td>
                                    <td>Most ML algorithms, error terms</td>
                                </tr>
                                <tr>
                                    <td>Exponential</td>
                                    <td>Continuous</td>
                                    <td>Always right-skewed</td>
                                    <td>Survival analysis, time-to-event</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="box">
                        <h4>Key Observations from Interactive Exploration:</h4>
                        <ul>
                            <li><strong>Binomial:</strong> With n=10 and p=0.5, distribution is symmetric and centered around 5. This explains why balanced datasets (equal positive/negative examples) are preferred in classification.</li>
                            <li><strong>Poisson:</strong> Confidence intervals widen over time, showing accumulating uncertainty—critical for time series forecasting.</li>
                            <li><strong>Hypergeometric:</strong> Unlike binomial, this accounts for changing probabilities as items are drawn without replacement—important for survey sampling.</li>
                            <li><strong>Normal:</strong> Adjusting μ shifts the curve horizontally; adjusting σ makes it wider/narrower. Most ML preprocessing involves standardization (mean=0, std=1).</li>
                            <li><strong>Exponential:</strong> Memoryless property means past waiting doesn't affect future—simplifies calculations but only applies when there's no "aging."</li>
                        </ul>
                    </div>

                    <h3>Analysis and Key Findings</h3>
                    <div class="box">
                        <p><strong>Why Distributions Matter in AI:</strong></p>
                        <ul>
                            <li><strong>Model Assumptions:</strong> Many ML algorithms assume specific distributions (e.g., linear regression assumes normal errors, Naive Bayes assumes independence)</li>
                            <li><strong>Anomaly Detection:</strong> Knowing expected distribution helps identify outliers</li>
                            <li><strong>Data Preprocessing:</strong> Understanding distributions informs scaling and transformation choices</li>
                            <li><strong>Uncertainty Quantification:</strong> Confidence intervals and prediction intervals rely on distributional assumptions</li>
                        </ul>
                    </div>

                    <div class="box">
                        <p><strong>Practical Applications I Learned:</strong></p>
                        <ul>
                            <li><strong>Binomial → A/B Testing:</strong> When comparing two website versions, we count successes in n trials. Binomial tells us if the difference is real or luck.</li>
                            <li><strong>Poisson → System Monitoring:</strong> How many errors per hour is normal? Poisson helps detect when something's wrong.</li>
                            <li><strong>Normal → Everything:</strong> Central Limit Theorem explains why normal appears everywhere—from test scores to measurement errors.</li>
                            <li><strong>Exponential → Queue Management:</strong> Time between customer arrivals, time until equipment fails.</li>
                        </ul>
                    </div>

                    <div class="box">
                        <p><strong>Parameter Sensitivity Insight:</strong></p>
                        <p>Playing with parameters showed me how sensitive distributions are. Small changes in λ, μ, or σ can dramatically change probabilities. This means:</p>
                        <ul>
                            <li>Accurate parameter estimation is critical</li>
                            <li>Wrong distribution choice leads to wrong conclusions</li>
                            <li>Understanding parameters helps interpret model behavior</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- ============================================ -->
            <!-- OVERALL LEARNING OUTCOMES -->
            <!-- ============================================ -->
            <section id="learning-outcomes-achieved">
                <div class="inner">
                    <header class="major">
                        <h2>⭐ Overall Learning Outcomes Achieved</h2>
                    </header>
                    
                    <h3>Technical Skills Developed:</h3>
                    <ul>
                        <li>Calculate basic probabilities and verify they sum to 1</li>
                        <li>Identify which distribution fits different scenarios</li>
                        <li>Understand what distribution parameters (μ, σ, λ, p, n) mean in practice</li>
                        <li>Recognize discrete vs continuous distributions and when to use each</li>
                        <li>Use interactive tools to build probabilistic intuition</li>
                    </ul>

                    <h3>Conceptual Understanding:</h3>
                    <ul>
                        <li><strong>Probability Axioms:</strong> Non-negativity, normalization, additivity</li>
                        <li><strong>Why balanced datasets matter:</strong> Binomial distribution is most predictable when p=0.5</li>
                        <li><strong>Why normal distribution is everywhere:</strong> Central Limit Theorem + many natural phenomena</li>
                        <li><strong>Why sampling matters:</strong> Poor sampling creates biased, failing models</li>
                        <li><strong>Parameter sensitivity:</strong> Small changes can dramatically affect predictions</li>
                    </ul>

                    <h3>AI/ML Applications:</h3>
                    <ul>
                        <li><strong>Classification:</strong> Binomial for success/failure outcomes, balanced datasets</li>
                        <li><strong>Anomaly Detection:</strong> Poisson for unusual event frequencies</li>
                        <li><strong>Regression:</strong> Normal distribution assumption for error terms</li>
                        <li><strong>Survival Analysis:</strong> Exponential for time-to-event modeling</li>
                        <li><strong>Bayesian ML:</strong> Prior and posterior distributions</li>
                        <li><strong>Data Quality:</strong> Sampling methods affect training data representativeness</li>
                    </ul>

                    <h3>Critical Thinking Skills:</h3>
                    <ul>
                        <li>Evaluate whether data meets distributional assumptions</li>
                        <li>Identify potential biases from sampling methods</li>
                        <li>Understand ethical implications of predictive models</li>
                        <li>Connect mathematical concepts to real-world scenarios</li>
                    </ul>
                </div>
            </section>

           <!-- Reflection -->
<section id="reflection">
    <div class="inner">
        <header class="major">
            <h2>Reflection</h2>
        </header>
        
        <p>This unit transformed probability from abstract formulas into something I can actually use. The marble activity seemed basic—just calculating 1/3 for each color—but it reinforced the core principle that probabilities must sum to 1. When ML models violate this, something's wrong.</p>
        
        <p>The interactive distribution exploration was the real game-changer. Watching the normal curve shift and widen as I adjusted μ and σ built intuition that formulas alone couldn't provide. Now when I see "standardization" in preprocessing, I understand it's transforming data to the standard normal distribution (mean=0, std=1).</p>
        
        <p>The crime prediction video raised an important point: probability helps optimize decisions, but if the underlying data is biased, we just optimize bias. This taught me that data scientists can't just chase metrics—we need to think critically about impact.</p>
        
        <p>Understanding sampling methods was eye-opening. Poor sampling creates biased models, period. The difference between stratified and simple random sampling directly affects whether training data represents reality. No sophisticated algorithm can fix biased data.</p>
        
        <p>What really clicked: probability isn't about perfect predictions—it's about making optimal decisions under uncertainty. The newsvendor model demonstrated this perfectly: even the best decision (Q=469) still expects 6 units understocked and 125 overstocked. That's not failure; that's probabilistic thinking.</p>
        
        <p>These foundations will underpin everything in my AI career: Bayesian methods, confidence intervals, hypothesis testing, Monte Carlo simulations. This unit wasn't just about learning distributions—it was about developing the probabilistic mindset needed to work with real-world uncertainty.</p>
    </div>
</section>


            <!-- Navigation -->
            <section id="navigation">
                <div class="inner">
                    <ul class="actions">
                        <li><a href="data-activity3.html" class="button">← Previous: Data Activity 3</a></li>
                        <li><a href="numerical-analysis.html" class="button">Back to Module Overview</a></li>
                        <li><a href="unit4-seminar.html" class="button">Next: Unit 4 Seminar →</a></li>
                    </ul>
                </div>
            </section>

        </div>

        <!-- Footer -->
        <footer id="footer">
            <div class="inner">
                <ul class="icons">
                    <li><a href="https://github.com/KannaAlmansoori" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
                </ul>
                <ul class="copyright">
                    <li>&copy; 2025 Kanna AlMansoori</li>
                    <li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
                </ul>
            </div>
        </footer>

    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

</body>
</html>"""

# Save the HTML file
with open('unit4-probability-activities.html', 'w', encoding='utf-8') as f:
    f.write(html_content)

print("✓ Reorganized version created with activities grouped together!")
print("File name: unit4-probability-activities.html")

# Download the file
from google.colab import files
files.download('unit4-probability-activities.html')
