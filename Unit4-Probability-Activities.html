html_content = """<!DOCTYPE HTML>
<html>
<head>
    <title>Unit 4 Probability Activities - Numerical Analysis - Kanna AlMansoori</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>
<body class="is-preload">

    <div id="wrapper">

        <!-- Header -->
        <header id="header">
            <a href="index.html" class="logo"><strong>Kanna AlMansoori</strong> <span>MSc Artificial Intelligence E-Portfolio</span></a>
            <nav>
                <a href="#menu">Menu</a>
            </nav>
        </header>

        <!-- Menu -->
        <nav id="menu">
            <ul class="links">
                <li><a href="index.html">Home</a></li>
                <li><a href="#about">About Me</a></li>
                <li><a href="launch-module.html">Launch into Computing</a></li>
                <li><a href="induction.html">Induction Computing</a></li>
                <li><a href="ai-module.html">Understanding Artificial Intelligence</a></li>
                <li><a href="numerical-analysis.html">Numerical Analysis</a></li>
                <li><a href="project.html">Project</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>

        <!-- Main Content -->
        <div id="main" class="alt">

            <!-- Breadcrumb Navigation -->
            <section id="one">
                <div class="inner">
                    <p><a href="numerical-analysis.html">← Back to Numerical Analysis</a></p>
                    <header class="major">
                        <h1>Unit 4: Probability Activities</h1>
                    </header>
                    <p><strong>Title:</strong> Exploring Fundamental Probability Concepts and Distributions | <strong>Deadline:</strong> End of Unit 4 | <strong>Type:</strong> Formative | <strong>Status:</strong> ✓ Complete</p>
                </div>
            </section>

            <!-- Learning Outcomes -->
            <section id="learning-outcomes">
                <div class="inner">
                    <header class="major">
                        <h2>Learning Outcomes from This Activity</h2>
                    </header>
                    <ul>
                        <li>Systematic understanding of the key mathematical and statistical concepts and techniques which underpin mechanisms in Data Science and AI.</li>
                        <li>Apply mathematical and statistical methods in these fields to help in the decision-making process.</li>
                        <li>Critically evaluate the use of statistical analysis and the numeric interpretation of results as aids in the decision-making process.</li>
                    </ul>
                </div>
            </section>

            <!-- Task Overview -->
            <section id="task-overview">
                <div class="inner">
                    <header class="major">
                        <h2>Task Overview</h2>
                    </header>
                    
                    <p>This unit introduced foundational probability concepts through three hands-on activities that build understanding from basic calculations to complex distributions.</p>
                    
                    <div class="box">
                        <h3>Three Activities Completed:</h3>
                        <ol>
                            <li><strong>Activity 1: Marble Probability</strong> - Calculate probabilities for selecting colored marbles and verify they sum to 1</li>
                            <li><strong>Activity 2: Real-World Applications</strong> - Watch videos on crime prediction and sampling methods</li>
                            <li><strong>Activity 3: Probability Distributions</strong> - Explore discrete and continuous distributions interactively</li>
                        </ol>
                    </div>
                </div>
            </section>

            <!-- ============================================ -->
            <!-- ACTIVITY 1: MARBLE PROBABILITY -->
            <!-- ============================================ -->
            <section id="activity1">
                <div class="inner">
                    <header class="major">
                        <h2>Activity 1: Marble Probability</h2>
                    </header>

                    <h3>Methodology</h3>
                    <p>Using the Wolfram demonstration website, I worked with an interactive marble probability simulator. The scenario had:</p>
                    <ul>
                        <li>1 red marble</li>
                        <li>1 blue marble</li>
                        <li>1 yellow marble</li>
                        <li><strong>Total: 3 marbles</strong></li>
                    </ul>
                    <p><strong>Probability Formula:</strong> P(event) = Number of favorable outcomes / Total number of possible outcomes</p>
                    <p>I calculated the probability of drawing each color, then verified that all probabilities sum to 1.</p>

                    <h3>Results</h3>
                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Question</th>
                                    <th>Calculation</th>
                                    <th>Answer</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Probability of getting a red marble</td>
                                    <td>P(red) = 1/3</td>
                                    <td>0.333 or 33.33%</td>
                                </tr>
                                <tr>
                                    <td>Probability of getting a blue marble</td>
                                    <td>P(blue) = 1/3</td>
                                    <td>0.333 or 33.33%</td>
                                </tr>
                                <tr>
                                    <td>Probability of getting a yellow marble</td>
                                    <td>P(yellow) = 1/3</td>
                                    <td>0.333 or 33.33%</td>
                                </tr>
                                <tr style="background-color: #f5f5f5;">
                                    <td><strong>Verification (sum of all probabilities)</strong></td>
                                    <td>1/3 + 1/3 + 1/3</td>
                                    <td><strong>1.0 or 100% ✓</strong></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h3>Analysis</h3>
                    <div class="box">
                        <p>This simple exercise reinforced three fundamental probability axioms: probabilities are always between 0 and 1 (non-negativity), they must sum to 1 (normalization), and with equal chances each outcome has probability 1/n (uniformity).</p>
                        <p><strong>Why this matters:</strong> When ML models output probabilities that don't sum to 1, that's a red flag. These basic rules constrain all probability models.</p>
                    </div>
                </div>
            </section>

            <!-- ============================================ -->
            <!-- ACTIVITY 2: REAL-WORLD APPLICATIONS -->
            <!-- ============================================ -->
            <section id="activity2">
                <div class="inner">
                    <header class="major">
                        <h2>Activity 2: Real-World Applications</h2>
                    </header>

                    <h3>Methodology</h3>
                    <p>Watched two educational videos:</p>
                    <ul>
                        <li><strong>"Crime spotting: Joy of Stats"</strong> - How probability is used in predictive policing and resource allocation</li>
                        <li><strong>"Types of Sampling Methods"</strong> - Different statistical sampling techniques (stratified, systematic, cluster, simple random)</li>
                    </ul>

                    <h3>Results</h3>
                    <div class="box">
                        <h4>Crime Prediction Insights:</h4>
                        <ul>
                            <li>Using probability to forecast where crimes are likely to occur</li>
                            <li>Deploying resources based on probability distributions</li>
                            <li>Same approach used in customer churn, equipment failure, medical diagnosis</li>
                        </ul>
                    </div>

                    <div class="box">
                        <h4>Sampling Methods Learned:</h4>
                        <ul>
                            <li><strong>Simple Random:</strong> Every member has equal selection probability</li>
                            <li><strong>Stratified:</strong> Divide into groups, sample from each (ensures representation)</li>
                            <li><strong>Systematic:</strong> Select every nth item</li>
                            <li><strong>Cluster:</strong> Randomly select entire groups</li>
                        </ul>
                    </div>

                    <h3>Analysis</h3>
                    <div class="box">
                        <p><strong>Ethical insight:</strong> The crime video raised an important point—using biased historical data can create self-fulfilling prophecies. Data scientists can't just optimize metrics; we must think critically about societal impacts.</p>
                        <p><strong>Critical lesson:</strong> Bad sampling = biased model. If training data isn't representative, the model will fail in production. Stratified sampling ensures all groups are represented—crucial for fair AI systems.</p>
                    </div>
                </div>
            </section>

            <!-- ============================================ -->
            <!-- ACTIVITY 3: PROBABILITY DISTRIBUTIONS -->
            <!-- ============================================ -->
            <section id="activity3">
                <div class="inner">
                    <header class="major">
                        <h2>Activity 3: Probability Distributions</h2>
                    </header>

                    <h3>Methodology</h3>
                    <p>Used interactive Jupyter notebooks to explore probability distributions by adjusting parameters and observing changes.</p>

                    <h4>Discrete Distributions:</h4>
                    <ul>
                        <li><strong>Binomial:</strong> Successes in fixed trials (parameters: n trials, p success probability)</li>
                        <li><strong>Poisson:</strong> Event counts over time (parameter: λ rate)</li>
                        <li><strong>Hypergeometric:</strong> Sampling without replacement (parameters: N population, k successes, n sample)</li>
                    </ul>

                    <h4>Continuous Distributions:</h4>
                    <ul>
                        <li><strong>Normal:</strong> Bell curve (parameters: μ mean, σ standard deviation)</li>
                        <li><strong>Exponential:</strong> Waiting times (parameter: λ rate, has memoryless property)</li>
                    </ul>

                    <h3>Results</h3>
                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Distribution</th>
                                    <th>Type</th>
                                    <th>Key Observation</th>
                                    <th>AI/ML Use</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Binomial</td>
                                    <td>Discrete</td>
                                    <td>Symmetric when p=0.5</td>
                                    <td>A/B testing, classification</td>
                                </tr>
                                <tr>
                                    <td>Poisson</td>
                                    <td>Discrete</td>
                                    <td>Confidence intervals widen over time</td>
                                    <td>Anomaly detection</td>
                                </tr>
                                <tr>
                                    <td>Hypergeometric</td>
                                    <td>Discrete</td>
                                    <td>Accounts for no replacement</td>
                                    <td>Survey sampling</td>
                                </tr>
                                <tr>
                                    <td>Normal</td>
                                    <td>Continuous</td>
                                    <td>σ controls spread</td>
                                    <td>Most ML algorithms</td>
                                </tr>
                                <tr>
                                    <td>Exponential</td>
                                    <td>Continuous</td>
                                    <td>Right-skewed, memoryless</td>
                                    <td>Survival analysis</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h3>Analysis</h3>
                    <div class="box">
                        <p><strong>Key insights from interactive exploration:</strong></p>
                        <ul>
                            <li><strong>Binomial symmetry:</strong> When p=0.5, distribution is perfectly balanced—explains why balanced datasets are preferred in classification</li>
                            <li><strong>Poisson uncertainty:</strong> Confidence intervals widening over time shows accumulating uncertainty in forecasting</li>
                            <li><strong>Normal ubiquity:</strong> Central Limit Theorem explains why this appears everywhere—from test scores to ML preprocessing (standardization to mean=0, std=1)</li>
                            <li><strong>Exponential memoryless property:</strong> Past waiting doesn't affect future—only works when there's no "aging"</li>
                        </ul>
                    </div>

                    <div class="box">
                        <p><strong>Practical applications:</strong></p>
                        <ul>
                            <li>Binomial → A/B testing (is the conversion difference real or luck?)</li>
                            <li>Poisson → System monitoring (how many errors per hour is normal?)</li>
                            <li>Normal → Regression errors, feature standardization</li>
                            <li>Exponential → Time until equipment fails, customer arrivals</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- ============================================ -->
            <!-- OVERALL LEARNING OUTCOMES -->
            <!-- ============================================ -->
            <section id="learning-outcomes-achieved">
                <div class="inner">
                    <header class="major">
                        <h2>Overall Learning Outcomes Achieved</h2>
                    </header>
                    
                    <h3>Technical Skills:</h3>
                    <ul>
                        <li>Calculate probabilities and verify they sum to 1</li>
                        <li>Identify appropriate distributions for different scenarios</li>
                        <li>Interpret distribution parameters (μ, σ, λ, p, n)</li>
                        <li>Distinguish between discrete and continuous distributions</li>
                    </ul>

                    <h3>Key Insights:</h3>
                    <ul>
                        <li><strong>Balanced datasets matter:</strong> Binomial is most predictable when p=0.5</li>
                        <li><strong>Normal distribution ubiquity:</strong> Central Limit Theorem explains its prevalence</li>
                        <li><strong>Sampling impacts models:</strong> Poor sampling creates biased results</li>
                        <li><strong>Parameter sensitivity:</strong> Small changes dramatically affect predictions</li>
                    </ul>

                    <h3>AI/ML Applications:</h3>
                    <ul>
                        <li>Binomial → Classification, A/B testing</li>
                        <li>Poisson → Anomaly detection, event counting</li>
                        <li>Normal → Regression errors, standardization</li>
                        <li>Exponential → Survival analysis, time-to-event</li>
                    </ul>
                </div>
            </section>

            <!-- ============================================ -->
            <!-- REFLECTION -->
            <!-- ============================================ -->
            <section id="reflection">
                <div class="inner">
                    <header class="major">
                        <h2>Reflection</h2>
                    </header>
                    
                    <p>This unit made probability real for me. The marble activity seemed basic—just calculating 1/3 for each color—but it reinforced that probabilities must always sum to 1. When ML models violate this, something's fundamentally wrong.</p>
                    
                    <p>The interactive distribution exploration was the game-changer. Watching the normal curve shift and widen as I adjusted μ and σ built intuition that equations alone couldn't give me. Now when I see "standardization" in preprocessing code, I actually understand what's happening—transforming data to mean=0 and std=1.</p>
                    
                    <p>The crime prediction video raised an uncomfortable truth: if your data is biased, you're just optimizing bias. This taught me that data scientists can't just chase metrics—we need to think about impact. Similarly, the sampling methods video made clear that poor sampling creates biased models, and no fancy algorithm can fix that.</p>
                    
                    <p>What really clicked: probability isn't about perfect predictions—it's about making optimal decisions under uncertainty. These foundations will underpin everything in AI: Bayesian methods, confidence intervals, Monte Carlo simulations. This wasn't just about learning distributions; it was about developing the probabilistic mindset needed to work with real-world uncertainty.</p>
                </div>
            </section>

            <!-- Navigation -->
            <section id="navigation">
                <div class="inner">
                    <ul class="actions">
                        <li><a href="data-activity3.html" class="button">← Previous: Data Activity 3</a></li>
                        <li><a href="numerical-analysis.html" class="button">Back to Module Overview</a></li>
                        <li><a href="unit4-seminar.html" class="button">Next: Unit 4 Seminar →</a></li>
                    </ul>
                </div>
            </section>

        </div>

        <!-- Footer -->
        <footer id="footer">
            <div class="inner">
                <ul class="icons">
                    <li><a href="https://github.com/KannaAlmansoori" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
                </ul>
                <ul class="copyright">
                    <li>&copy; 2025 Kanna AlMansoori</li>
                    <li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
                </ul>
            </div>
        </footer>

    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

</body>
</html>"""

ties.html')
